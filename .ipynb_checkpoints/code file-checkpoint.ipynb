{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57f6dbcf-9b25-4670-bf28-466242963b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/manudev/anaconda3/lib/python3.10/site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/manudev/anaconda3/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/manudev/anaconda3/lib/python3.10/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/manudev/anaconda3/lib/python3.10/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/manudev/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08fe939d-4a19-4ae8-a0fe-4deb5b68d07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # (data preprocessing,CSV file I/O (e.g) pd.read_csv)\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a74c6c4-b50e-4619-a205-84265ee76655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>notebook_id</th>\n",
       "      <th>date_evaluated</th>\n",
       "      <th>tier</th>\n",
       "      <th>total_execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eca89c0554</td>\n",
       "      <td>2022-04-08T07:47:07.470Z</td>\n",
       "      <td>MASTER</td>\n",
       "      <td>5573.173455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>021bbc7ee2</td>\n",
       "      <td>2022-08-02T09:49:22.587Z</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>152.800169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a2137a2ae8</td>\n",
       "      <td>2022-04-28T06:09:43.927Z</td>\n",
       "      <td>EXPERT</td>\n",
       "      <td>3110.543580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a655fbe4b8</td>\n",
       "      <td>2022-02-06T05:44:17.220Z</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>74.077047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1b318124e3</td>\n",
       "      <td>2021-09-23T00:27:19.700Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.374717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2953</th>\n",
       "      <td>87db7e7138</td>\n",
       "      <td>2022-08-04T03:53:21.637Z</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>38.732182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2954</th>\n",
       "      <td>1ff1de7740</td>\n",
       "      <td>2022-08-14T09:49:15.753Z</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>1268.031509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2955</th>\n",
       "      <td>6492d38d73</td>\n",
       "      <td>2020-12-13T17:02:32.010Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>348.246678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2956</th>\n",
       "      <td>8216124282</td>\n",
       "      <td>2021-09-30T17:11:49.553Z</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>296.508680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2957</th>\n",
       "      <td>838e8afb1c</td>\n",
       "      <td>2022-03-28T19:51:47.957Z</td>\n",
       "      <td>EXPERT</td>\n",
       "      <td>694.680458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2958 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     notebook_id            date_evaluated         tier  total_execution_time\n",
       "0     eca89c0554  2022-04-08T07:47:07.470Z       MASTER           5573.173455\n",
       "1     021bbc7ee2  2022-08-02T09:49:22.587Z  CONTRIBUTOR            152.800169\n",
       "2     a2137a2ae8  2022-04-28T06:09:43.927Z       EXPERT           3110.543580\n",
       "3     a655fbe4b8  2022-02-06T05:44:17.220Z  CONTRIBUTOR             74.077047\n",
       "4     1b318124e3  2021-09-23T00:27:19.700Z          NaN             13.374717\n",
       "...          ...                       ...          ...                   ...\n",
       "2953  87db7e7138  2022-08-04T03:53:21.637Z  CONTRIBUTOR             38.732182\n",
       "2954  1ff1de7740  2022-08-14T09:49:15.753Z  CONTRIBUTOR           1268.031509\n",
       "2955  6492d38d73  2020-12-13T17:02:32.010Z          NaN            348.246678\n",
       "2956  8216124282  2021-09-30T17:11:49.553Z  CONTRIBUTOR            296.508680\n",
       "2957  838e8afb1c  2022-03-28T19:51:47.957Z       EXPERT            694.680458\n",
       "\n",
       "[2958 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('info_train.csv')\n",
    "test = pd.read_csv('info_test.csv')\n",
    "sub = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "datasrc = pd.read_csv('datasources.csv')\n",
    "cdtrain = pd.read_csv('codes_train.csv')\n",
    "cdtest = pd.read_csv('codes_test.csv')\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3af68722-dc00-4cec-8d04-82a46562aa9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>notebook_id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>total_columns</th>\n",
       "      <th>total_rows</th>\n",
       "      <th>file_extension</th>\n",
       "      <th>file_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cfcd208495</td>\n",
       "      <td>Breast_Cancer</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4024.0</td>\n",
       "      <td>.csv</td>\n",
       "      <td>396119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c4ca4238a0</td>\n",
       "      <td>sample_submission</td>\n",
       "      <td>4.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>.csv</td>\n",
       "      <td>1914.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c4ca4238a0</td>\n",
       "      <td>train</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11218.0</td>\n",
       "      <td>.csv</td>\n",
       "      <td>471056.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c4ca4238a0</td>\n",
       "      <td>tf_efficientnet_b5_ap-456</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>.pt</td>\n",
       "      <td>114218400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c4ca4238a0</td>\n",
       "      <td>timm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>.tgz</td>\n",
       "      <td>22647078.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48183</th>\n",
       "      <td>bcfa8a783a</td>\n",
       "      <td>train</td>\n",
       "      <td>82.0</td>\n",
       "      <td>21263.0</td>\n",
       "      <td>.csv</td>\n",
       "      <td>23859780.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48184</th>\n",
       "      <td>f1920129f9</td>\n",
       "      <td>gender_submission</td>\n",
       "      <td>2.0</td>\n",
       "      <td>418.0</td>\n",
       "      <td>.csv</td>\n",
       "      <td>3258.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48185</th>\n",
       "      <td>f1920129f9</td>\n",
       "      <td>test</td>\n",
       "      <td>11.0</td>\n",
       "      <td>418.0</td>\n",
       "      <td>.csv</td>\n",
       "      <td>28629.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48186</th>\n",
       "      <td>f1920129f9</td>\n",
       "      <td>train</td>\n",
       "      <td>12.0</td>\n",
       "      <td>891.0</td>\n",
       "      <td>.csv</td>\n",
       "      <td>61194.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48187</th>\n",
       "      <td>f1920129f9</td>\n",
       "      <td>titanic_submission_9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>.csv</td>\n",
       "      <td>2839.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48188 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      notebook_id                  file_name  total_columns  total_rows  \\\n",
       "0      cfcd208495              Breast_Cancer           16.0      4024.0   \n",
       "1      c4ca4238a0          sample_submission            4.0        64.0   \n",
       "2      c4ca4238a0                      train            4.0     11218.0   \n",
       "3      c4ca4238a0  tf_efficientnet_b5_ap-456            NaN         NaN   \n",
       "4      c4ca4238a0                       timm            NaN         NaN   \n",
       "...           ...                        ...            ...         ...   \n",
       "48183  bcfa8a783a                      train           82.0     21263.0   \n",
       "48184  f1920129f9          gender_submission            2.0       418.0   \n",
       "48185  f1920129f9                       test           11.0       418.0   \n",
       "48186  f1920129f9                      train           12.0       891.0   \n",
       "48187  f1920129f9       titanic_submission_9            NaN         NaN   \n",
       "\n",
       "      file_extension    file_size  \n",
       "0               .csv     396119.0  \n",
       "1               .csv       1914.0  \n",
       "2               .csv     471056.0  \n",
       "3                .pt  114218400.0  \n",
       "4               .tgz   22647078.0  \n",
       "...              ...          ...  \n",
       "48183           .csv   23859780.0  \n",
       "48184           .csv       3258.0  \n",
       "48185           .csv      28629.0  \n",
       "48186           .csv      61194.0  \n",
       "48187           .csv       2839.0  \n",
       "\n",
       "[48188 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60c74068-b405-4dd3-a951-ec7b8e03cae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>notebook_id</th>\n",
       "      <th>code_cell</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c4ca4238a0</td>\n",
       "      <td>0</td>\n",
       "      <td>!mkdir -p ../work\\n\\n!cd ../work &amp;&amp; tar xfz .....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c4ca4238a0</td>\n",
       "      <td>1</td>\n",
       "      <td>import os\\n\\nimport numpy as np\\n\\nimport pand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c4ca4238a0</td>\n",
       "      <td>2</td>\n",
       "      <td>debug = True\\n\\nif debug:\\n\\n    epochs = 3\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c4ca4238a0</td>\n",
       "      <td>3</td>\n",
       "      <td>df = pd.read_csv(\"../input/dfl-bundesliga-data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c4ca4238a0</td>\n",
       "      <td>4</td>\n",
       "      <td>def extract_training_images(args):\\n\\n        ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52752</th>\n",
       "      <td>6562c5c1f3</td>\n",
       "      <td>16</td>\n",
       "      <td>train, test = train_test_split(scaled_data, te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52753</th>\n",
       "      <td>6562c5c1f3</td>\n",
       "      <td>17</td>\n",
       "      <td>residual = pd.DataFrame(model_fit.resid)\\n\\nre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52754</th>\n",
       "      <td>6562c5c1f3</td>\n",
       "      <td>18</td>\n",
       "      <td>preds = model_fit.predict(start='1959-10-01', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52755</th>\n",
       "      <td>6562c5c1f3</td>\n",
       "      <td>19</td>\n",
       "      <td>from sklearn.metrics import r2_score, mean_squ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52756</th>\n",
       "      <td>6562c5c1f3</td>\n",
       "      <td>20</td>\n",
       "      <td>test_set = np.exp(test)\\n\\npredictions = np.ex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52757 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      notebook_id  code_cell  \\\n",
       "0      c4ca4238a0          0   \n",
       "1      c4ca4238a0          1   \n",
       "2      c4ca4238a0          2   \n",
       "3      c4ca4238a0          3   \n",
       "4      c4ca4238a0          4   \n",
       "...           ...        ...   \n",
       "52752  6562c5c1f3         16   \n",
       "52753  6562c5c1f3         17   \n",
       "52754  6562c5c1f3         18   \n",
       "52755  6562c5c1f3         19   \n",
       "52756  6562c5c1f3         20   \n",
       "\n",
       "                                                  source  \n",
       "0      !mkdir -p ../work\\n\\n!cd ../work && tar xfz .....  \n",
       "1      import os\\n\\nimport numpy as np\\n\\nimport pand...  \n",
       "2      debug = True\\n\\nif debug:\\n\\n    epochs = 3\\n\\...  \n",
       "3      df = pd.read_csv(\"../input/dfl-bundesliga-data...  \n",
       "4      def extract_training_images(args):\\n\\n        ...  \n",
       "...                                                  ...  \n",
       "52752  train, test = train_test_split(scaled_data, te...  \n",
       "52753  residual = pd.DataFrame(model_fit.resid)\\n\\nre...  \n",
       "52754  preds = model_fit.predict(start='1959-10-01', ...  \n",
       "52755  from sklearn.metrics import r2_score, mean_squ...  \n",
       "52756  test_set = np.exp(test)\\n\\npredictions = np.ex...  \n",
       "\n",
       "[52757 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ec54f8f-fa9e-41bd-892e-d2338bc77bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>notebook_id</th>\n",
       "      <th>code_cell</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49079</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>0</td>\n",
       "      <td>import os\\n\\nfor dirname, _, filenames in os.w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49080</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>1</td>\n",
       "      <td>#Load data-preprocessing libraries\\n\\nimport p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49081</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>2</td>\n",
       "      <td>#Load training data\\n\\ndf_data=pd.read_csv('/k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49082</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>3</td>\n",
       "      <td>#Load test data\\n\\ndf_test=pd.read_csv('/kaggl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49083</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>4</td>\n",
       "      <td>#Function to rename the columns to get a bette...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49084</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>5</td>\n",
       "      <td>#Rename train data\\n\\nrename_dataframe(df_data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49085</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>6</td>\n",
       "      <td>#Rename test data\\n\\nrename_dataframe(df_test)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49086</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>7</td>\n",
       "      <td>#shape of data\\n\\nprint('Train data shape: ',d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49087</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>8</td>\n",
       "      <td>#percentage of missing data\\n\\n(df_data.isnull...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49088</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>9</td>\n",
       "      <td>#As only 0.91% of data in Tweet features are n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49089</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>10</td>\n",
       "      <td>#check percentage of missing data after droppi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49090</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>11</td>\n",
       "      <td>df_data.info()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49091</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>12</td>\n",
       "      <td>print('Distinct Values: \\n')\\n\\ncol=['Topic','...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49092</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>13</td>\n",
       "      <td>#Visualizing Sentiment feature - Target featur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49093</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>14</td>\n",
       "      <td>#Get the count of words in each tweet\\n\\ndf_da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49094</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>15</td>\n",
       "      <td>plt.figure(figsize=(15,10))\\n\\nplt.subplot(2,1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49095</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>16</td>\n",
       "      <td>#Extreme outliers\\n\\nextreme_outliers = df_dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49096</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>17</td>\n",
       "      <td>#Get count of characters in each tweet excludi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49097</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>18</td>\n",
       "      <td>plt.figure(figsize=(15,10))\\n\\nplt.subplot(2,1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49098</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49099</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>20</td>\n",
       "      <td>#Remove user mentions from the tweets\\n\\n#'I w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49100</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>21</td>\n",
       "      <td>#Remove hashtags from the tweets\\n\\n#'My new h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49101</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>22</td>\n",
       "      <td># make a dictionary of contractions\\n\\ncontrac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49102</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>23</td>\n",
       "      <td>#Remove contractions\\n\\ndf_data['Tweet_clean']...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49103</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>24</td>\n",
       "      <td>#Remove links/urls from the tweets\\n\\ndf_data[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49104</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>25</td>\n",
       "      <td>def txt_conversion(sentence):\\n\\n    #Getting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49105</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>26</td>\n",
       "      <td>df_data['Tweet_clean']=df_data['Tweet_clean']....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49106</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>27</td>\n",
       "      <td>#Removing stop-words and cpnverting words to l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49107</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>28</td>\n",
       "      <td>df_data['Tweet_clean']=df_data['Tweet_clean']....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49108</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>29</td>\n",
       "      <td>#Drop columns that are not required\\n\\ndf_data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49109</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>30</td>\n",
       "      <td>df_data.dropna(axis=0,how='any',inplace=True)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49110</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>31</td>\n",
       "      <td>plt.figure(figsize=(15,10))\\n\\n\\n\\nplt.subplot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49111</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49112</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>33</td>\n",
       "      <td>#Seperate dependent and independent features\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49113</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>34</td>\n",
       "      <td>#TF-IDF\\n\\nvectorizer = TfidfVectorizer(stop_w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49114</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>35</td>\n",
       "      <td>#Function to fit and apply a model\\n\\ndef mode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49115</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>36</td>\n",
       "      <td>#Multinomail Naive Bayes\\n\\nnb=MultinomialNB()...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49116</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>37</td>\n",
       "      <td>#Logistic Regression\\n\\nlr=LogisticRegression(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49117</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>38</td>\n",
       "      <td>#Decision Tree\\n\\ndtc=DecisionTreeClassifier(r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49118</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>39</td>\n",
       "      <td>#Random Forest\\n\\nrf=RandomForestClassifier(ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49119</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      notebook_id  code_cell  \\\n",
       "49079  86dba86754          0   \n",
       "49080  86dba86754          1   \n",
       "49081  86dba86754          2   \n",
       "49082  86dba86754          3   \n",
       "49083  86dba86754          4   \n",
       "49084  86dba86754          5   \n",
       "49085  86dba86754          6   \n",
       "49086  86dba86754          7   \n",
       "49087  86dba86754          8   \n",
       "49088  86dba86754          9   \n",
       "49089  86dba86754         10   \n",
       "49090  86dba86754         11   \n",
       "49091  86dba86754         12   \n",
       "49092  86dba86754         13   \n",
       "49093  86dba86754         14   \n",
       "49094  86dba86754         15   \n",
       "49095  86dba86754         16   \n",
       "49096  86dba86754         17   \n",
       "49097  86dba86754         18   \n",
       "49098  86dba86754         19   \n",
       "49099  86dba86754         20   \n",
       "49100  86dba86754         21   \n",
       "49101  86dba86754         22   \n",
       "49102  86dba86754         23   \n",
       "49103  86dba86754         24   \n",
       "49104  86dba86754         25   \n",
       "49105  86dba86754         26   \n",
       "49106  86dba86754         27   \n",
       "49107  86dba86754         28   \n",
       "49108  86dba86754         29   \n",
       "49109  86dba86754         30   \n",
       "49110  86dba86754         31   \n",
       "49111  86dba86754         32   \n",
       "49112  86dba86754         33   \n",
       "49113  86dba86754         34   \n",
       "49114  86dba86754         35   \n",
       "49115  86dba86754         36   \n",
       "49116  86dba86754         37   \n",
       "49117  86dba86754         38   \n",
       "49118  86dba86754         39   \n",
       "49119  86dba86754         40   \n",
       "\n",
       "                                                  source  \n",
       "49079  import os\\n\\nfor dirname, _, filenames in os.w...  \n",
       "49080  #Load data-preprocessing libraries\\n\\nimport p...  \n",
       "49081  #Load training data\\n\\ndf_data=pd.read_csv('/k...  \n",
       "49082  #Load test data\\n\\ndf_test=pd.read_csv('/kaggl...  \n",
       "49083  #Function to rename the columns to get a bette...  \n",
       "49084  #Rename train data\\n\\nrename_dataframe(df_data...  \n",
       "49085  #Rename test data\\n\\nrename_dataframe(df_test)...  \n",
       "49086  #shape of data\\n\\nprint('Train data shape: ',d...  \n",
       "49087  #percentage of missing data\\n\\n(df_data.isnull...  \n",
       "49088  #As only 0.91% of data in Tweet features are n...  \n",
       "49089  #check percentage of missing data after droppi...  \n",
       "49090                                     df_data.info()  \n",
       "49091  print('Distinct Values: \\n')\\n\\ncol=['Topic','...  \n",
       "49092  #Visualizing Sentiment feature - Target featur...  \n",
       "49093  #Get the count of words in each tweet\\n\\ndf_da...  \n",
       "49094  plt.figure(figsize=(15,10))\\n\\nplt.subplot(2,1...  \n",
       "49095  #Extreme outliers\\n\\nextreme_outliers = df_dat...  \n",
       "49096  #Get count of characters in each tweet excludi...  \n",
       "49097  plt.figure(figsize=(15,10))\\n\\nplt.subplot(2,1...  \n",
       "49098                                                NaN  \n",
       "49099  #Remove user mentions from the tweets\\n\\n#'I w...  \n",
       "49100  #Remove hashtags from the tweets\\n\\n#'My new h...  \n",
       "49101  # make a dictionary of contractions\\n\\ncontrac...  \n",
       "49102  #Remove contractions\\n\\ndf_data['Tweet_clean']...  \n",
       "49103  #Remove links/urls from the tweets\\n\\ndf_data[...  \n",
       "49104  def txt_conversion(sentence):\\n\\n    #Getting ...  \n",
       "49105  df_data['Tweet_clean']=df_data['Tweet_clean']....  \n",
       "49106  #Removing stop-words and cpnverting words to l...  \n",
       "49107  df_data['Tweet_clean']=df_data['Tweet_clean']....  \n",
       "49108  #Drop columns that are not required\\n\\ndf_data...  \n",
       "49109      df_data.dropna(axis=0,how='any',inplace=True)  \n",
       "49110  plt.figure(figsize=(15,10))\\n\\n\\n\\nplt.subplot...  \n",
       "49111                                                NaN  \n",
       "49112  #Seperate dependent and independent features\\n...  \n",
       "49113  #TF-IDF\\n\\nvectorizer = TfidfVectorizer(stop_w...  \n",
       "49114  #Function to fit and apply a model\\n\\ndef mode...  \n",
       "49115  #Multinomail Naive Bayes\\n\\nnb=MultinomialNB()...  \n",
       "49116  #Logistic Regression\\n\\nlr=LogisticRegression(...  \n",
       "49117  #Decision Tree\\n\\ndtc=DecisionTreeClassifier(r...  \n",
       "49118  #Random Forest\\n\\nrf=RandomForestClassifier(ra...  \n",
       "49119                                                NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdtest[cdtest['notebook_id']=='86dba86754']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6ceacec-3a14-49f9-9e5f-d47e987f0b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>notebook_id</th>\n",
       "      <th>total_execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86dba86754</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57aeee35c9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33267e5dc5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cf67355a33</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>831caa1b60</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589</th>\n",
       "      <td>9d068c869f</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590</th>\n",
       "      <td>6b5754d737</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1591</th>\n",
       "      <td>d91d1b4d82</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592</th>\n",
       "      <td>61f2585b0e</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>a67f096809</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1594 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     notebook_id  total_execution_time\n",
       "0     86dba86754                   0.0\n",
       "1     57aeee35c9                   0.0\n",
       "2     33267e5dc5                   0.0\n",
       "3     cf67355a33                   0.0\n",
       "4     831caa1b60                   0.0\n",
       "...          ...                   ...\n",
       "1589  9d068c869f                   0.0\n",
       "1590  6b5754d737                   0.0\n",
       "1591  d91d1b4d82                   0.0\n",
       "1592  61f2585b0e                   0.0\n",
       "1593  a67f096809                   0.0\n",
       "\n",
       "[1594 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "274c5c6b-046e-4ec4-9ac3-ecae945d96a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['df_numeric = df.loc[:, numeric_list]\\n\\nsns.pairplot(df_numeric, hue = \"output\", diag_kind = \"kde\")\\n\\nplt.show()',\n",
       "       \"FileLinks('.')\\n\",\n",
       "       \"# extracting titles from Name column.\\n\\nfor dataset in train_test_data:\\n\\n    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\\\.')\",\n",
       "       \"def odt_plots(df,col):\\n\\n    f,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(20,20))\\n\\n    \\n\\n    #box plot for descriptive stats\\n\\n    \\n\\n    sns.boxplot(df[col],ax=ax1,color='red')\\n\\n    ax1.set_title(col+ '  '+'Boxplot')\\n\\n    ax1.set_xlabel('Boxplot')\\n\\n    ax1.set_ylabel('Values')\\n\\n    \\n\\n    \\n\\n    \\n\\n    #plotting histogram with outliers\\n\\n    \\n\\n    sns.distplot(df[col],ax=ax2,color='blue')\\n\\n    ax2.set_title(col+ '  '+'Histogram with outliers')\\n\\n    ax2.set_xlabel('Histogram')\\n\\n    ax2.set_ylabel('Values')\\n\\n    \\n\\n    #plotting histogram after treating outliers\\n\\n    \\n\\n    y = replace_outlier(df,col)\\n\\n    \\n\\n    sns.distplot(y[col],ax=ax3,color='green')\\n\\n    ax3.set_title(col+ '  '+'Histogram without outliers')\\n\\n    ax3.set_xlabel('Histogram')\\n\\n    ax3.set_ylabel('Values')\\n\\n    \",\n",
       "       \"plt.hist(pred, alpha=0.4, label='prediction')\\n\\nplt.hist(valid_set['y'], alpha=0.4, label='actual target')\\n\\nplt.legend(frameon=False)\",\n",
       "       \"# Model Evaluation on test data\\n\\n\\n\\n# Summary of the predictions made by the classifier\\n\\nprint(classification_report(y_test,pred_test))\\n\\nprint(confusion_matrix(y_test,pred_test))\\n\\n\\n\\n# Accuracy score\\n\\nprint('Accuracy Score of Model on test data is: {}' .format(accuracy_score(y_test,pred_test)))\",\n",
       "       \"eda.Sex.replace({0:'male',1:'female'}, inplace=True)\\n\\neda.Education.replace({0 :'other / unknown',1: 'high school',2: 'university',3: 'graduate school'}, inplace=True)\\n\\neda['Marital status'].replace({0 :'single',1 :'non-single'}, inplace=True)\\n\\neda.Occupation.replace({0 :'unemployed',1: 'official',2: 'management / self-employed'}, inplace=True)\\n\\neda['Settlement size'].replace({0: 'small city',1: 'mid-sized city',2: 'big city'}, inplace=True)\",\n",
       "       '# using the model for prediction\\n\\n\\n\\nsent = \"\"\"आप कितना सोचते हो\\n\\nअगर आप ठिठुरती रातों को गिनें\\n\\nअरे क्या आप मिल सकते हैं (अरे, क्या आप मिल सकते हैं?)\\n\\nक्या तुम मिलोगे (क्या तुम मिलोगे?)\\n\\nसर्दियों का अंत बताओ\\n\\nएक कोमल वसंत के दिन तक\\n\\nमैं चाहता हूं कि तुम तब तक रहो जब तक फूल खिल न जाएं\\n\\nज्यों का त्यों\"\"\"\\n\\n\\n\\n\\n\\nsent = cv.transform([sent])\\n\\nans = model.predict(sent)\\n\\nans = np.argmax(ans)\\n\\nle.inverse_transform([ans])',\n",
       "       \"import gc\\n\\nimport os\\n\\nimport sys\\n\\nimport time\\n\\nimport pickle\\n\\nimport random\\n\\nfrom tqdm import tqdm\\n\\nimport numpy as np\\n\\nimport pandas as pd\\n\\nfrom tqdm.notebook import tqdm\\n\\nfrom sklearn.metrics import log_loss\\n\\nfrom sklearn.model_selection import StratifiedKFold\\n\\n\\n\\nimport torch\\n\\nimport transformers\\n\\nimport torch.nn as nn\\n\\nimport torch.nn.functional as F\\n\\nfrom torch.cuda.amp import GradScaler, autocast\\n\\nfrom torch.utils.data import Dataset, DataLoader\\n\\nfrom transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\\n\\n\\n\\nimport warnings\\n\\nwarnings.simplefilter('ignore')\",\n",
       "       'df.info()',\n",
       "       \"# removing unnecessary columns\\n\\n\\n\\ncol = ['Unnamed: 0', 'salary', 'salary_currency']\\n\\ndf = salary.drop(col, axis = 1,)\",\n",
       "       \"from sklearn.naive_bayes import GaussianNB\\n\\n\\n\\nnb_G = GaussianNB()\\n\\nnb_G.fit(xlr_train,ylr_train)\\n\\n\\n\\nynb_hat = nb_G.predict(xlr_test)\\n\\n\\n\\nco_ma = confusion_matrix(ylr_test,ynb_hat)\\n\\nprint('C-M:', co_ma)\\n\\nprint(classification_report(ylr_test,ynb_hat))\\n\\n\\n\\n\\n\\nfpr, tpr, threshold = metrics.roc_curve(ylr_test, ynb_hat);\\n\\nroc_auc = metrics.auc(fpr, tpr);\\n\\n\\n\\nplt.title('Receiver Operating Characteristic');\\n\\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc);\\n\\nplt.legend(loc = 'lower right');\\n\\nplt.plot([0, 1], [0, 1],'r--');\\n\\nplt.xlim([0, 1]);\\n\\nplt.ylim([0, 1]);\\n\\nplt.ylabel('True Positive Rate');\\n\\nplt.xlabel('False Positive Rate');\",\n",
       "       '# Create a SHAP Dependence plot of the 2nd highest ranked feature - \"Product_Info_4\".\\n\\n\\n\\nshap.dependence_plot(\"rank(1)\", shap_values[1], X_test_L1reg)',\n",
       "       'df_preprocessed.isnull().mean() * 100',\n",
       "       'print(\"Memory usage for object col (as category dtype) in bytes :\" ,\\n\\n      df[\\'object\\'].astype(\\'category\\').memory_usage(deep=True))',\n",
       "       'plt.figure(figsize=(30,14))\\n\\nsns.boxplot(data=raw_data)\\n\\nplt.show()',\n",
       "       \"df = pd.read_csv('../input/heart-attack-analysis-prediction-dataset/heart.csv')\\n\\ndf\",\n",
       "       \"student_score = []\\n\\n\\n\\nfor course in preparation_course:\\n\\n    reading_score_course = students_performance['reading score'][students_performance['test preparation course'] == course]\\n\\n    mean_reading_score_course = int(reading_score_course.mean())\\n\\n    student_score.append(mean_reading_score_course)\\n\\n\\n\\nmean_reading_preparation_courses = pd.DataFrame(data=[student_score], columns=preparation_course)\\n\\n\\n\\nsns.barplot(data=mean_reading_preparation_courses);\",\n",
       "       \" data[data['gender']=='Other'].index\",\n",
       "       'def find_local_minima(volume, height, width):\\n\\n    \"\"\"\\n\\n    Finds the two smallest local minima in the \\n\\n    cost volume for all pixels.\\n\\n\\n\\n    Arguments:\\n\\n        - volume: Array containing the matching costs for \\n\\n            all pixels at all disparities, with dimension H x W x D.\\n\\n        - height: number of rows of the image.\\n\\n        - width: number of columns of the image.\\n\\n        \\n\\n    Returns: The global mimimum costs, c1, for all pixels with dimension H x W, \\n\\n        and the second local minimum costs, c2m, for all pixels with dimension H x W.\\n\\n    \"\"\"\\n\\n\\n\\n    c1 = np.zeros(shape=(height, width), dtype=np.int32)\\n\\n    c2m = np.zeros(shape=(height, width), dtype=np.int32)\\n\\n\\n\\n    for y in range(height):\\n\\n        for x in range(width):\\n\\n            minimum = np.min(volume[y, x, :])\\n\\n            maximum = np.max(volume[y, x, :])\\n\\n            # Get local minima indices\\n\\n            local_min_disparities = argrelextrema(volume[y, x, :], np.less)[0]\\n\\n            # Get local minima values\\n\\n            local_min_costs = np.take(volume[y, x, :], local_min_disparities)\\n\\n            local_min_costs = np.sort(local_min_costs)\\n\\n            # If there of no troughs, then the values are either flat or monotonic. \\n\\n            # In either case we can take c1 to be the min cost and c2m as the max.\\n\\n            if local_min_costs.size < 1:\\n\\n                c1[y, x] = minimum\\n\\n                c2m[y, x] = maximum\\n\\n            # If there is only one trough, then the trough is either the local min or global min.\\n\\n            # If the trough cost is equal to the min of all values, \\n\\n            # then the trough is the global min, otherwise its a local min.\\n\\n            elif local_min_costs.size < 2:\\n\\n                if local_min_costs[0] == minimum:\\n\\n                    c1[y, x] = local_min_costs[0]\\n\\n                    c2m[y, x] = maximum\\n\\n                else:\\n\\n                    c1[y, x] = minimum\\n\\n                    c2m[y, x] = local_min_costs[0]\\n\\n            # Ideal case when we have two or more troughs, then we can just take the smallest two troughs\\n\\n            else:\\n\\n                c1[y, x] = local_min_costs[0]\\n\\n                c2m[y, x] = local_min_costs[1]\\n\\n    return c1, c2m\\n'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdtrain['source'].sample(20).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6afcc003-b5a1-4179-b04c-ab91ebd4da51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['notebook_id', 'code_cell', 'source', 'execution_time'], dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdtrain.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a3363e0-98e9-4f76-81e7-90620236a472",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/manudev/nltk_data'\n    - '/home/manudev/anaconda3/nltk_data'\n    - '/home/manudev/anaconda3/share/nltk_data'\n    - '/home/manudev/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/home/manudev/nltk_data'\n    - '/home/manudev/anaconda3/nltk_data'\n    - '/home/manudev/anaconda3/share/nltk_data'\n    - '/home/manudev/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:38\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:14\u001b[0m, in \u001b[0;36mcreate_feats\u001b[0;34m(df_review)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/manudev/nltk_data'\n    - '/home/manudev/anaconda3/nltk_data'\n    - '/home/manudev/anaconda3/share/nltk_data'\n    - '/home/manudev/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import string\n",
    "punctuation=string.punctuation\n",
    "\n",
    "def create_feats(df_review):\n",
    "    df_review['source'].fillna('None',inplace=True)\n",
    "    df_review['word_count']=df_review['source'].apply(lambda x: len(str(x).split(\" \")))\n",
    "    df_review['char_count'] = df_review['source'].str.len()\n",
    "    def avg_word(sentence):\n",
    "        words = sentence.split()\n",
    "        return (sum(len(word) for word in words)/(len(words)+1))\n",
    "\n",
    "    df_review['avg_word'] = df_review['source'].apply(lambda x: avg_word(x))\n",
    "    from nltk.corpus import stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "    df_review['stopwords'] = df_review['source'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "    df_review['numerics'] = df_review['source'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "    df_review['upper'] = df_review['source'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "    df_review['word_density'] = df_review['char_count'] / (df_review['word_count']+1)\n",
    "    df_review['punctuation_count'] = df_review['source'].apply(lambda x: len(\"\".join(_ for _ in x if _ in punctuation))) \n",
    "\n",
    "    df_review['identifiers'] = df_review['source'].apply(lambda x: x.count('#')+x.count('@')+x.count('http'))\n",
    "    df_review['os'] = df_review['source'].apply(lambda x: x.count('!')+x.count('pip')+x.count('os'))\n",
    "    df_review['import'] = df_review['source'].apply(lambda x: x.count('import'))\n",
    "    df_review['plot'] = df_review['source'].apply(lambda x: x.count('plot')+x.count('sns.')+x.count('plt.'))\n",
    "    df_review['deeplr'] = df_review['source'].apply(lambda x: x.count('keras.')+x.count('torch.'))\n",
    "    df_review['train_pred'] = df_review['source'].apply(lambda x: x.count('fit')+x.count('predict')+x.count('predict_proba'))\n",
    "    df_review['model'] = df_review['source'].apply(lambda x: x.count('Regressor')+x.count('Classifier')+x.count('Kmeans'))\n",
    "    df_review.drop(['source'],axis=1,inplace=True)\n",
    "    return df_review\n",
    "\n",
    "\n",
    "\n",
    "cols = ['notebook_id', 'code_cell', 'source', 'execution_time']\n",
    "\n",
    "\n",
    "df = cdtrain.append(cdtest,ignore_index=True)\n",
    "df = create_feats(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48daf837-563b-464f-91b5-acab5a56de0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a90296-5349-407b-92c5-585e68019aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
